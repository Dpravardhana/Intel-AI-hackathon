{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fastforwardlabs/whisper-openai/blob/master/WhisperDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "\n",
        "<img src=\"https://blog.fastforwardlabs.com/images/cloudera-fast-forward-logo.png\" width=300>\n",
        "\n",
        "# Make your own recordings and transcriptions with OpenAI's Whisper!\n",
        "_a fun diversion brought to you by [Melanie](https://www.linkedin.com/in/melanierbeck/), ML Research Manager at Cloudera Fast Forward Labs_\n",
        "\n",
        "\n",
        "[OpenAI](https://openai.com/) [recently released](https://openai.com/blog/whisper/) Whisper, an automatic speech recognition (ASR) system that was trained on a colossal heap of audio data collected from the web. \n",
        "\n",
        "## What makes Whisper unique? \n",
        "Speech-to-text technology isn't new but Whisper might usher in the  next-generation of ASR systems in terms of the quality and capabilities delivered by a single model (rather than a collection of models, as most ASR systems are today.)  So what makes Whisper special? \n",
        "\n",
        "1. It's \"weakly supervised,\" meaning that it was trained on (audio, transcript) pairs wherein the transcriptions were _not_ human-validated, a typical hallmark of gold-stardard supervised audio datasets.  \n",
        "2. This allows Whisper to be trained on a MASSIVE amount of data scraped from the web (680K hours) -- orders of magnitude larger than most audio datasets (5-30K hours)\n",
        "3. The data contains audio snippets in dozens of languages allowing Whisper to be natively multilingual. \n",
        "4. Whisper is also multitask -- it can perform transcription, translation, voice detection, and language detection. \n",
        "\n",
        "You can learn more about Whisper in OpenAI's [recent post]((https://openai.com/blog/whisper/)), their [paper](https://cdn.openai.com/papers/whisper.pdf), and their open-source [codebase](https://github.com/openai/whisper). In fact, this notebook made use of OpenAI's [LibriSpeech](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb) Colab example as a starting point, so you should check that out as well!\n",
        "\n",
        "\n",
        "This notebook allows you to play with Whisper by first creating your own audio recording, processing the audio into a format the model will understand, and feeding it into Whisper for translation or transcription! Let's get started. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLdnbL4iRpvB"
      },
      "source": [
        "## Installs and imports \n",
        "The commands below will install the Python packages needed to record audio snippets and use Whisper models for speech-to-text transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZsJUxc0aRsAf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to c:\\users\\harec\\appdata\\local\\temp\\pip-req-build-du6xyzhk\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting numba\n",
            "  Using cached numba-0.59.1-cp311-cp311-win_amd64.whl (2.6 MB)\n",
            "Requirement already satisfied: numpy in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.1-cp311-cp311-win_amd64.whl (198.6 MB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Collecting more-itertools\n",
            "  Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0\n",
            "  Using cached llvmlite-0.42.0-cp311-cp311-win_amd64.whl (28.1 MB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from torch->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "Collecting jinja2\n",
            "  Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "Requirement already satisfied: fsspec in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from torch->openai-whisper==20231117) (2024.3.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Collecting mpmath>=0.19\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml): started\n",
            "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=812161 sha256=3608c89d196e04ede1dabf027626d41eb07b5346f4eb810e67c02d86ef73f9ed\n",
            "  Stored in directory: C:\\Users\\harec\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2fz0u99q\\wheels\\1f\\1d\\98\\9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: mpmath, sympy, networkx, more-itertools, llvmlite, jinja2, torch, tiktoken, numba, openai-whisper\n",
            "Successfully installed jinja2-3.1.3 llvmlite-0.42.0 more-itertools-10.2.0 mpmath-1.3.0 networkx-3.2.1 numba-0.59.1 openai-whisper-20231117 sympy-1.12 tiktoken-0.6.0 torch-2.2.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\harec\\AppData\\Local\\Temp\\pip-req-build-du6xyzhk'\n",
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sounddevice\n",
            "  Using cached sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
            "Collecting wavio\n",
            "  Using cached wavio-0.0.8-py3-none-any.whl (9.4 kB)\n",
            "Collecting CFFI>=1.0\n",
            "  Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from wavio) (1.26.4)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Installing collected packages: wavio, pycparser, CFFI, sounddevice\n",
            "Successfully installed CFFI-1.16.0 pycparser-2.21 sounddevice-0.4.6 wavio-0.0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywebrtc\n",
            "  Using cached ipywebrtc-0.6.0-py2.py3-none-any.whl (260 kB)\n",
            "Collecting notebook\n",
            "  Using cached notebook-7.1.2-py3-none-any.whl (5.0 MB)\n",
            "Collecting jupyter-server<3,>=2.4.0\n",
            "  Downloading jupyter_server-2.13.0-py3-none-any.whl (383 kB)\n",
            "     -------------------------------------- 383.2/383.2 kB 2.4 MB/s eta 0:00:00\n",
            "Collecting jupyterlab-server<3,>=2.22.1\n",
            "  Downloading jupyterlab_server-2.25.4-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 59.0/59.0 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting jupyterlab<4.2,>=4.1.1\n",
            "  Downloading jupyterlab-4.1.5-py3-none-any.whl (11.4 MB)\n",
            "     ---------------------------------------- 11.4/11.4 MB 5.3 MB/s eta 0:00:00\n",
            "Collecting notebook-shim<0.3,>=0.2\n",
            "  Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from notebook) (6.4)\n",
            "Collecting anyio>=3.1.0\n",
            "  Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
            "Collecting argon2-cffi\n",
            "  Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.3)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.7.2)\n",
            "Collecting jupyter-events>=0.9.0\n",
            "  Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Collecting jupyter-server-terminals\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Collecting nbconvert>=6.4.4\n",
            "  Downloading nbconvert-7.16.3-py3-none-any.whl (257 kB)\n",
            "     -------------------------------------- 257.4/257.4 kB 5.3 MB/s eta 0:00:00\n",
            "Collecting nbformat>=5.3.0\n",
            "  Downloading nbformat-5.10.3-py3-none-any.whl (78 kB)\n",
            "     ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
            "Collecting overrides\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (24.0)\n",
            "Collecting prometheus-client\n",
            "  Downloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
            "     ---------------------------------------- 54.5/54.5 kB 1.4 MB/s eta 0:00:00\n",
            "Collecting pywinpty\n",
            "  Downloading pywinpty-2.0.13-cp311-none-win_amd64.whl (1.4 MB)\n",
            "     ---------------------------------------- 1.4/1.4 MB 5.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyzmq>=24 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (25.1.2)\n",
            "Collecting send2trash>=1.8.2\n",
            "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
            "Collecting terminado>=0.8.3\n",
            "  Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.14.2)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 58.5/58.5 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting async-lru>=1.0.0\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Collecting httpx>=0.25.0\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Requirement already satisfied: ipykernel in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook) (6.29.3)\n",
            "Collecting jupyter-lsp>=2.0.0\n",
            "  Downloading jupyter_lsp-2.2.4-py3-none-any.whl (69 kB)\n",
            "     ---------------------------------------- 69.1/69.1 kB 1.9 MB/s eta 0:00:00\n",
            "Collecting babel>=2.10\n",
            "  Downloading Babel-2.14.0-py3-none-any.whl (11.0 MB)\n",
            "     ---------------------------------------- 11.0/11.0 MB 5.1 MB/s eta 0:00:00\n",
            "Collecting json5>=0.9.0\n",
            "  Downloading json5-0.9.24-py3-none-any.whl (30 kB)\n",
            "Collecting jsonschema>=4.18.0\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "     ---------------------------------------- 85.5/85.5 kB 2.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests>=2.31 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.6)\n",
            "Collecting sniffio>=1.1\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook) (2024.2.2)\n",
            "Collecting httpcore==1.*\n",
            "  Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "Collecting h11<0.15,>=0.13\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook) (2.1.5)\n",
            "Collecting attrs>=22.2.0\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
            "Collecting jsonschema-specifications>=2023.03.6\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Collecting referencing>=0.28.4\n",
            "  Downloading referencing-0.34.0-py3-none-any.whl (26 kB)\n",
            "Collecting rpds-py>=0.7.1\n",
            "  Downloading rpds_py-0.18.0-cp311-none-win_amd64.whl (206 kB)\n",
            "     -------------------------------------- 206.7/206.7 kB 6.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (4.2.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (306)\n",
            "Collecting python-json-logger>=2.0.4\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (6.0.1)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "     -------------------------------------- 147.9/147.9 kB 4.3 MB/s eta 0:00:00\n",
            "Collecting bleach!=5.0.0\n",
            "  Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
            "     -------------------------------------- 162.8/162.8 kB 4.8 MB/s eta 0:00:00\n",
            "Collecting defusedxml\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting mistune<4,>=2.0.3\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "     ---------------------------------------- 48.0/48.0 kB 2.4 MB/s eta 0:00:00\n",
            "Collecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.10.0-py3-none-any.whl (25 kB)\n",
            "Collecting pandocfilters>=1.4.1\n",
            "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.17.2)\n",
            "Collecting tinycss2\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Collecting fastjsonschema\n",
            "  Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook) (2.2.1)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
            "Requirement already satisfied: comm>=0.1.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (1.8.1)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (8.22.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (1.6.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (5.9.8)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.16.0)\n",
            "Collecting webencodings\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: decorator in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.19.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (3.0.43)\n",
            "Requirement already satisfied: stack-data in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.6.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.4.6)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook) (1.16.0)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pycparser in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook) (2.21)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.2.13)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "     ---------------------------------------- 66.4/66.4 kB 1.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\harec\\llm_prac\\.env\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<4.2,>=4.1.1->notebook) (0.2.2)\n",
            "Collecting types-python-dateutil>=2.8.10\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: webencodings, ipywebrtc, fastjsonschema, websocket-client, webcolors, uri-template, types-python-dateutil, tinycss2, soupsieve, sniffio, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, pywinpty, python-json-logger, prometheus-client, pandocfilters, overrides, mistune, jupyterlab-pygments, jsonpointer, json5, h11, fqdn, defusedxml, bleach, babel, attrs, async-lru, terminado, referencing, httpcore, beautifulsoup4, arrow, argon2-cffi-bindings, anyio, jupyter-server-terminals, jsonschema-specifications, isoduration, httpx, argon2-cffi, jsonschema, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook\n",
            "Successfully installed anyio-4.3.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.4 attrs-23.2.0 babel-2.14.0 beautifulsoup4-4.12.3 bleach-6.1.0 defusedxml-0.7.1 fastjsonschema-2.19.1 fqdn-1.5.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 ipywebrtc-0.6.0 isoduration-20.11.0 json5-0.9.24 jsonpointer-2.4 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 jupyter-events-0.10.0 jupyter-lsp-2.2.4 jupyter-server-2.13.0 jupyter-server-terminals-0.5.3 jupyterlab-4.1.5 jupyterlab-pygments-0.3.0 jupyterlab-server-2.25.4 mistune-3.0.2 nbclient-0.10.0 nbconvert-7.16.3 nbformat-5.10.3 notebook-7.1.2 notebook-shim-0.2.4 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.20.0 python-json-logger-2.0.7 pywinpty-2.0.13 referencing-0.34.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.18.0 send2trash-1.8.2 sniffio-1.3.1 soupsieve-2.5 terminado-0.18.1 tinycss2-1.2.1 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.7.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install sounddevice wavio\n",
        "! pip install ipywebrtc notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfgdT6iHnLC"
      },
      "source": [
        "We also need the following in order to record audio from this notebook and process the resulting files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wIRFnTn3Fzua"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!apt install ffmpeg\n",
        "!apt-get install libportaudio2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3CqtR2Fi5-vP"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "from IPython.display import Audio, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IMEkgyagYto"
      },
      "source": [
        "## Make your recording\n",
        "\n",
        "First, we need to enable some Colab widgets so that we can make an audio recording. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmCVlVnMAGQX"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4pSZkqTH8jR"
      },
      "source": [
        "### Time to record! \n",
        "\n",
        "Press the circle button and start speaking. It may not look it, but the widget will be capturing sound. Click the circle button again when you are finished. The widget will immediately begin to play back what it captured. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107,
          "referenced_widgets": [
            "09446a03c33742dfa70a9f242f96b3be",
            "8964df95ded44ee28b7ed225c564ed9b",
            "823fe8b97ef94aedaed6889ac580c8eb",
            "5604b41fde3b45dd80b954c6128bccf7",
            "09e1f5b2de9945aea20f6129b7c82ec9",
            "35775e7c3c5846a589410566cbec95fa"
          ]
        },
        "id": "-fFdSBBAGjFk",
        "outputId": "5894a254-7fe0-4593-fbee-74491cd72b9f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09446a03c33742dfa70a9f242f96b3be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': …"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "recorder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk-zZpPBG25R"
      },
      "source": [
        "The audio format captured above is not readable by PyTorch. In this step, we convert our recording into a format that PyTorch can understand. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDDgAohMGrCR"
      },
      "outputs": [],
      "source": [
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav my_recording.wav -y -hide_banner -loglevel panic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQJhMt6pIgJd"
      },
      "source": [
        "### Alternatively... \n",
        "If you don't want to make your own recording, you can instead upload an audio file to this notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8m2ez4lGrWn"
      },
      "source": [
        "## Select options\n",
        "\n",
        "Whisper is capable of performing transcriptions for many languages (though it performs better for some languages and worse for others.) \n",
        "\n",
        "Whisper is also capable of detecting the input language. However, to be on the safe side, we can explicitly tell Whisper which language to expect. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7p2AoJItnIM"
      },
      "outputs": [],
      "source": [
        "language_options = whisper.tokenizer.TO_LANGUAGE_CODE \n",
        "language_list = list(language_options.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8c4d89ec973647d1a46aa471311e037c",
            "6c5f2af50210411f801f812dc17c389c",
            "90728ace9c454a3ab05ffb3e0bb664a3",
            "b50eded046cf4d378b1d71a186995d21"
          ]
        },
        "id": "dpLnKvlb-vLa",
        "outputId": "bd012b8a-d413-41a9-834a-6674c0e2928a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c4d89ec973647d1a46aa471311e037c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(options=('english', 'chinese', 'german', 'spanish', 'russian', 'korean', 'french', 'japanese', 'portu…"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "lang_dropdown = widgets.Dropdown(options=language_list, value='english')\n",
        "output = widgets.Output()\n",
        "display(lang_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EipQBR-INZOW"
      },
      "source": [
        "Whisper is also capable of several tasks, including English-only transcription, Any-to-English translation, and non-English transcription. \n",
        "\n",
        "Below you can select either \"transcription\" (which will yield text in the same language as the input language) or \"translation\" (which will transcribe from non-English to English). \n",
        "\n",
        "![Whisper capabilities](https://cdn.openai.com/whisper/draft-20220920a/asr-training-data-desktop.svg)\n",
        "\n",
        "Image from [Introducing Whisper](https://openai.com/blog/whisper/) by OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "43cdc270c46644b6a0308d3c499601fb",
            "5f9cd9708efb486c930c1397be9a566c",
            "20cdb2181c804215b4a1e6006be77734",
            "fb4b05bd474c409288394cd82d6a9179"
          ]
        },
        "id": "ilyDW-ALMnke",
        "outputId": "4a34e1f6-519c-46e5-a00b-05fbe3540e18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43cdc270c46644b6a0308d3c499601fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(options=('transcribe', 'translate'), value='transcribe')"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "task_dropdown = widgets.Dropdown(options=['transcribe', 'translate'], value='transcribe')\n",
        "output = widgets.Output()\n",
        "display(task_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG8FN6TQ_ale"
      },
      "source": [
        "## Load Whisper model\n",
        "\n",
        "Whisper comes in five model sizes, four of which also have an optimized English-only version. This notebook loads \"base\"-sized models (bigger than \"tiny\" but smaller than the others), which require about 1GB of RAM. \n",
        "\n",
        "If you selected English above, the cell below will load the optimized English-only version. Otherwise, it will load the multilingual model. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PokfNJtOYNu",
        "outputId": "227e41ec-b1a5-409d-c3c4-20d4564fb09c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:09<00:00, 14.9MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is English-only and has 71,825,408 parameters.\n"
          ]
        }
      ],
      "source": [
        "if lang_dropdown.value == \"english\":\n",
        "  model = whisper.load_model(\"base.en\")\n",
        "else:\n",
        "  model = whisper.load_model(\"base\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNrLhTEqP-Q0"
      },
      "source": [
        "Finally, let's set the rest of our task and language options below and see what we've got. Check that your task and language settings are correct, but don't worry about the other defaults. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwOUHau-dkUt",
        "outputId": "d16eef87-3af5-43e6-b54e-407a2e32f5d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecodingOptions(task='transcribe', language='english', temperature=0.0, sample_len=None, best_of=None, beam_size=None, patience=None, length_penalty=None, prompt=None, prefix=None, suppress_blank=True, suppress_tokens='-1', without_timestamps=True, max_initial_timestamp=0.0, fp16=True)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "options = whisper.DecodingOptions(language=lang_dropdown.value, task=task_dropdown.value, without_timestamps=True)\n",
        "options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns_iOuQpJEHk"
      },
      "source": [
        "## Take Whisper for a test drive\n",
        "\n",
        "All that's left to do now is feed our audio into Whisper. \n",
        "\n",
        "The cell below performs the last processing steps to make this happen. First, it loads our PyTorch-ready audio file. Then it pads the audio into 30 sec segments. It creates a log-mel spectrogram of the audio and this is fed into Whisper along with the options we set above. \n",
        "\n",
        "\n",
        "\n",
        "_Note: if you chose to upload your own audio file rather than create one through this notebook, you'll need to update the audio filename below._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6S0VvoK0vfq"
      },
      "outputs": [],
      "source": [
        "audio = whisper.load_audio(\"my_recording.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "result = model.decode(mel, options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Mg64_MWW1uMb",
        "outputId": "444cfab3-f2bd-4519-9779-3f4aba72d1cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The tree was in Virginia. It was a long way away from anywhere on the back of an old farm. To get to the farm they had to drive for almost an hour south from Blacksburg, to drive roads with names like Pennywinkle Branch and Rooster Spur. They got turned around twice and Mr. Nancy and Chernobyl both lost their tempers with shadow and with each other.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cHDLtquJNsH"
      },
      "source": [
        "### How well did Whisper do??\n",
        "\n",
        "I read aloud the snippet above from one of my favorite books. For the record, Whisper is pretty dang close, making only two small mistakes -- both on proper names.   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09446a03c33742dfa70a9f242f96b3be": {
          "model_module": "jupyter-webrtc",
          "model_module_version": "~0.6.0",
          "model_name": "AudioRecorderModel",
          "state": {
            "_data_src": "blob:https://nfrp4p17vqk-496ff2e9c6d22116-0-colab.googleusercontent.com/d984436f-3337-406a-97dd-76a1145ef36f",
            "_dom_classes": [],
            "_model_module": "jupyter-webrtc",
            "_model_module_version": "~0.6.0",
            "_model_name": "AudioRecorderModel",
            "_view_count": null,
            "_view_module": "jupyter-webrtc",
            "_view_module_version": "~0.6.0",
            "_view_name": "AudioRecorderView",
            "audio": "IPY_MODEL_8964df95ded44ee28b7ed225c564ed9b",
            "autosave": false,
            "codecs": "",
            "filename": "record",
            "format": "webm",
            "layout": "IPY_MODEL_823fe8b97ef94aedaed6889ac580c8eb",
            "recording": false,
            "stream": "IPY_MODEL_5604b41fde3b45dd80b954c6128bccf7"
          }
        },
        "09e1f5b2de9945aea20f6129b7c82ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20cdb2181c804215b4a1e6006be77734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35775e7c3c5846a589410566cbec95fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43cdc270c46644b6a0308d3c499601fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "transcribe",
              "translate"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_5f9cd9708efb486c930c1397be9a566c",
            "style": "IPY_MODEL_20cdb2181c804215b4a1e6006be77734"
          }
        },
        "5604b41fde3b45dd80b954c6128bccf7": {
          "model_module": "jupyter-webrtc",
          "model_module_version": "~0.6.0",
          "model_name": "CameraStreamModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "jupyter-webrtc",
            "_model_module_version": "~0.6.0",
            "_model_name": "CameraStreamModel",
            "_view_count": null,
            "_view_module": "jupyter-webrtc",
            "_view_module_version": "~0.6.0",
            "_view_name": "MediaStreamView",
            "constraints": {
              "audio": true,
              "video": false
            },
            "layout": "IPY_MODEL_35775e7c3c5846a589410566cbec95fa"
          }
        },
        "5f9cd9708efb486c930c1397be9a566c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5f2af50210411f801f812dc17c389c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "823fe8b97ef94aedaed6889ac580c8eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8964df95ded44ee28b7ed225c564ed9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "AudioModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "AudioModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "AudioView",
            "autoplay": true,
            "controls": true,
            "format": "webm",
            "layout": "IPY_MODEL_09e1f5b2de9945aea20f6129b7c82ec9",
            "loop": true
          }
        },
        "8c4d89ec973647d1a46aa471311e037c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "english",
              "chinese",
              "german",
              "spanish",
              "russian",
              "korean",
              "french",
              "japanese",
              "portuguese",
              "turkish",
              "polish",
              "catalan",
              "dutch",
              "arabic",
              "swedish",
              "italian",
              "indonesian",
              "hindi",
              "finnish",
              "vietnamese",
              "hebrew",
              "ukrainian",
              "greek",
              "malay",
              "czech",
              "romanian",
              "danish",
              "hungarian",
              "tamil",
              "norwegian",
              "thai",
              "urdu",
              "croatian",
              "bulgarian",
              "lithuanian",
              "latin",
              "maori",
              "malayalam",
              "welsh",
              "slovak",
              "telugu",
              "persian",
              "latvian",
              "bengali",
              "serbian",
              "azerbaijani",
              "slovenian",
              "kannada",
              "estonian",
              "macedonian",
              "breton",
              "basque",
              "icelandic",
              "armenian",
              "nepali",
              "mongolian",
              "bosnian",
              "kazakh",
              "albanian",
              "swahili",
              "galician",
              "marathi",
              "punjabi",
              "sinhala",
              "khmer",
              "shona",
              "yoruba",
              "somali",
              "afrikaans",
              "occitan",
              "georgian",
              "belarusian",
              "tajik",
              "sindhi",
              "gujarati",
              "amharic",
              "yiddish",
              "lao",
              "uzbek",
              "faroese",
              "haitian creole",
              "pashto",
              "turkmen",
              "nynorsk",
              "maltese",
              "sanskrit",
              "luxembourgish",
              "myanmar",
              "tibetan",
              "tagalog",
              "malagasy",
              "assamese",
              "tatar",
              "hawaiian",
              "lingala",
              "hausa",
              "bashkir",
              "javanese",
              "sundanese",
              "burmese",
              "valencian",
              "flemish",
              "haitian",
              "letzeburgesch",
              "pushto",
              "panjabi",
              "moldavian",
              "moldovan",
              "sinhalese",
              "castilian"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_6c5f2af50210411f801f812dc17c389c",
            "style": "IPY_MODEL_90728ace9c454a3ab05ffb3e0bb664a3"
          }
        },
        "90728ace9c454a3ab05ffb3e0bb664a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b50eded046cf4d378b1d71a186995d21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb4b05bd474c409288394cd82d6a9179": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
